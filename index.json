[{"authors":["yuzhuo-fu"],"categories":null,"content":"个人主页\n付宇卓，教授，博士生导师。\n博士毕业于哈尔滨工业大学计算机体系结构专业，现就职于上海交通大学电子信息与电气工程学院微纳电子系教授。\n1990年获得国防科技大学计算机系学士学位，1997年和2001分别获得哈尔滨工业大学计算机系统结构专业工学硕士和博士学位。2006年获教授职称，2010年获博导资格，华盛顿大学、康考迪亚大学访问学者、加拿大Concordia大学客座教授。\n发表国内外高水平论文100余篇，担任多个国际会议、核心期刊的审稿人。主持过国家863、上海市科委等课题，参与国家核、高、基重大科研项目，获得过2008年上海市领军人才荣誉，多次荣获上海交通大学优秀教师称号。\n担任国家集成电路人才培养基地委员会委员、上海交通大学电类学部学位委员会委员、上海交通大学国家集成电路人才培养基地负责人、上海交通大学教学指导委员会委员、上海交通大学实践教学委员会主任委员；上海集成电路行业协会副理事长单位上海交通大学负责人、上海市科委领域预测专家、上海市经信委聘任专家、闵行区科委专家、紫竹科技园集成电路平台建设专家。\nProf. Yuzhuo Fu\nVice director of student Innovation center, and leading the Innovative Computer Architecture and Technology Lab (iCAT) at Dept. of Micro/Nano Electronics, SJTU.\nHe received a B.S. degree from the Computer Engineering Department at Changsha Institute of Technology, and M.S. and Ph.D. degrees from the Computer Science and Engineering Department, Harbin Institute of Technology. Before joining SJTU in 2001, he worked as a Senior Engineer for the electric engineering institute of Heilongjiang University and Computing Center of Heilongjiang Province.\nHe is a Present Deputy Professor of the undergraduate student affairs office of SJTU.\nHis research interests include fault-tolerant architecture, heterogeneous system architecture, and edge computing architecture. Especially focused on application-driven design/architecture innovations, which include novel architectures for artificial intelligence (AI) and acceleration with CPU/GPU/FPGA.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"f02af2e35003a4fce8419938885e8c32","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"个人主页 付宇卓，教授，博士生导师。 博士毕业于哈尔滨工业大学计","tags":null,"title":"付宇卓","type":"authors"},{"authors":["ting-liu"],"categories":null,"content":"个人主页\n上海交通大学微电子学院软件工程硕士。曾参与微电子火工品系统控制总线协议设计和多核片上系统仿真平台的FPGA验证等项目的研发。\n主要研究方向包括基于FPGA的多核片上系统架构设计，多核架构下的系统功耗模型分析，片上网络架构及协议设计与验证等。\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"c26560187b7aae67ede0cbb7b3f6d1f5","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"个人主页 上海交通大学微电子学院软件工程硕士。曾参与微电子火工","tags":null,"title":"刘婷","type":"authors"},{"authors":["suncheng-xiang"],"categories":null,"content":"Hi, I am Suncheng Xiang, a final-year PhD student at Shanghai Jiao Tong University.\nBefore this, I graduated from Changsha University of Science \u0026amp; Technology (CSUST) for B.Eng in Electrical Engineering and Automation in 2014, then obtained the M.Eng from National University of Defense Technology (NUDT) in 2017.\nMy main research topics are computer vision and machine learning. Specifically, I am currently doing research works on person re-identification, domain adaptation, transfer learning and related applications. I am also interested in some image generation and image retrieval tasks, e.g. vehicle re-identification and fine-grained image retrieval tasks.\nThis is my Homepage.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"91425658346a37c7627888b5a2c7c0f6","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Hi, I am Suncheng Xiang, a final-year PhD student at Shanghai Jiao Tong University. Before this, I graduated from Changsha University of Science \u0026amp; Technology (CSUST) for B.Eng in Electrical","tags":null,"title":"向孙程","type":"authors"},{"authors":["zefang-yu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"8d85dd6b83d65856fb3e0add1bc61a4d","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"于泽芳","type":"authors"},{"authors":["mingye-xie"],"categories":null,"content":"I have received the B. Sc. degree from East China Normal University, China, in 2016, the M. Eng. Degree in electronics and communications engineering from East China Normal University, China, in 2019.\nI am currently pursing Ph. D. degree in computer science and technology from Shanghai Jiao Tong University, China.\nMy research interests include Generative Adversial Network (GAN) and model compressing.\nFurther information can check my Homepage.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"8913f79c0868d79a409c8ff959e200e9","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I have received the B. Sc. degree from East China Normal University, China, in 2016, the M. Eng. Degree in electronics and communications engineering from East China Normal University, China,","tags":null,"title":"谢铭烨","type":"authors"},{"authors":["wei-ran"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1718064000,"objectID":"a9790e59ba40e33e74534081e26117da","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"冉苇","type":"authors"},{"authors":["jingsheng-gao"],"categories":null,"content":"​ I am a first-year PhD student at SJTU EECS, advised by Prof. Yuzhuo Fu. Previously, I obtained my bachelor\u0026rsquo;s degree from school of Physics and Astromony, Shanghai Jiaotong University.\n​ I completed my undergraduate graduation project using different kinds of U-nets, which is a astronomical task called extracting weak gravitational lens in CMB. Also, I have ever practiced in a medical company which is established by Fosun. During my first year, I have tried some projects on document understanding, like seal extraction and tabel detection.\n​ In the future, I will mainly put my attention on computer version and try to catch the future development of machine learning.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"6f881165b4c0103ea74df6299fd980f0","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"​ I am a first-year PhD student at SJTU EECS, advised by Prof. Yuzhuo Fu. Previously, I obtained my bachelor\u0026rsquo;s degree from school of Physics and Astromony, Shanghai Jiaotong University.","tags":null,"title":"高景盛","type":"authors"},{"authors":[null],"categories":null,"content":"None\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"4036ab9376e924db20b9e42fb1307811","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"None","tags":null,"title":"","type":"authors"},{"authors":["binjie-yan"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1707976805,"objectID":"12faef7d774d27c48ef33b87daab73ac","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"闫彬洁","type":"authors"},{"authors":["jiacheng-ruan"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"c02afb82bb3ace28aade6f87659e605d","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"阮佳程","type":"authors"},{"authors":["lining-hu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"477d2ae37505dcfc23372cf0e25d606a","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"胡立宁","type":"authors"},{"authors":["lin-xu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1707976805,"objectID":"13bffac5dfb607ac9f0fd041fac5882b","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"徐琳","type":"authors"},{"authors":["xian-gao"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f734440d7af507705fa9502d96445625","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"高显","type":"authors"},{"authors":["lin-xu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"671e5b184349c86b2bb0dd289e261027","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"袁文镇","type":"authors"},{"authors":["zongyu-zhang"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"1f4cf8d686ee0ce359e794d5856484d7","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"张宗云","type":"authors"},{"authors":["yanping-hu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1676419200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1676419200,"objectID":"d5b4aa2c078fe36926f2c874bcdfaf38","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"胡燕萍","type":"authors"},{"authors":["admin"],"categories":null,"content":"网站管理员。\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"网站管理员。","tags":null,"title":"iCAT","type":"authors"},{"authors":["tong-yu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"fadaa8758bda1bb2c1a5a13190f4c1f3","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"于潼","type":"authors"},{"authors":["阮佳程","高景盛","谢铭烨","向孙程","于泽芳","刘婷","付宇卓"],"categories":[],"content":"","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733356800,"objectID":"241508c8fa7c7890a40b0a69939dc6ca","permalink":"http://localhost:1313/publication/ruan2024gist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2024gist/","section":"publication","summary":"The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or introduces fewer trainable parameters to calibrate pre-trained models on downstream tasks, has become a recent research interest. However, existing PEFT methods within the traditional fine-tiuning framework have two main shortcomings: 1) They overlook the explicit association between trainable parameters and downstream task knowledge. 2) They neglect the interaction between the intrinsic task-agnostic knowledge of pre-trained models and the task-specific knowledge in downstream tasks. To address this gap, we propose a novel fine-tuning framework, named GIST, in a plug-and-play manner. Specifically, our framework first introduces a trainable token, called the Gist token, when applying PEFT methods on downstream tasks. This token serves as an aggregator of the task-specific knowledge learned by the PEFT methods and forms an explicit association with downstream knowledge. Furthermore, to facilitate explicit interaction between task-agnostic and task-specific knowledge, we introduce the concept of Knowledge Interaction via a Bidirectional Kullback-Leibler Divergence objective. As a result, PEFT methods within our framework can make the pretrained model understand downstream tasks more comprehensively by leveraging the knowledge interaction. Extensive experiments demonstrate the universality and scalability of our framework. Notably, on the VTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our GIST framework and achieve a performance boost of 2.25%, with an increase of only 0.8K parameters (0.01‰ of ViT-B/16).","tags":[],"title":"GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction","type":"publication"},{"authors":["于泽芳","谢铭烨","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1722988800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722988800,"objectID":"1184d4360b56297e4dc9c1911756ea1b","permalink":"http://localhost:1313/publication/yu2024raw/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/yu2024raw/","section":"publication","summary":"Understanding student behavior in educational settings is critical in improving both the quality of pedagogy and the level of student engagement. While various AI-based models exist for classroom analysis, they tend to specialize in limited tasks and lack generalizability across diverse educational environments. Additionally, these models often fall short in ensuring student privacy and in providing actionable insights accessible to educators. To bridge this gap, we introduce a unifed, end-to-end framework by leveraging temporal action detection techniques and advanced large language models for a more nuanced student behavior analysis. Our proposed framework provides an end-to-end pipeline that start with raw classroom video footage and culminates in the autonomous generation of pedagogical reports. It offers a comprehensive and scalable solution for student behavior analysis. Experimental validation confrms the capability of our framework to accurately identify student behaviors and to produce pedagogically meaningful insights, thereby setting the stage for future AI-assisted educational assessments.","tags":[],"title":"From Raw Video to Pedagogical Insights: A Unified Framework for Student Behavior Analysis","type":"publication"},{"authors":["高景盛","阮佳程","向孙程","于泽芳","谢铭烨","刘婷","付宇卓"],"categories":[],"content":"","date":1722902400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722902400,"objectID":"aaabef15de319334f2abe21214e1e5cc","permalink":"http://localhost:1313/publication/gao2024lamm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2024lamm/","section":"publication","summary":"With the success of pre-trained visual-language (VL) models such as CLIP in visual representation tasks, transferring pre-trained models to downstream tasks has become a crucial paradigm. Recently, the prompt tuning paradigm, which draws inspiration from natural language processing (NLP), has made significant progress in VL field. However, preceding methods mainly focus on constructing prompt templates for text and visual inputs, neglecting the gap in class label representations between the VL models and downstream tasks. To address this challenge, we introduce an innovative label alignment method named LAMM, which can dynamically adjust the category embeddings of downstream datasets through end-to-end training. Moreover, to achieve a more appropriate label distribution, we propose a hierarchical loss, encompassing the alignment of the parameter space, feature space, and logits space. We conduct experiments on 11 downstream vision datasets and demonstrate that our method significantly improves the performance of existing multi-modal prompt learning models in few-shot scenarios, exhibiting an average accuracy improvement of 2.31(%) compared to the state-of-the-art methods on 16 shots. Moreover, our methodology exhibits the preeminence in continual learning compared to other prompt tuning methods. Importantly, our method is synergistic with existing prompt tuning methods and can boost the performance on top of them. Our code and dataset will be publicly available at https://github.com/gaojingsheng/LAMM.","tags":[],"title":"LAMM: Label Alignment for Multi-Modal Prompt Learning","type":"publication"},{"authors":["高景盛","Zeyu Li","向孙程","Zhuowei Wang","刘婷","付宇卓"],"categories":[],"content":"","date":1722902400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722902400,"objectID":"4b6e62694022f5873d46788f26d89c1d","permalink":"http://localhost:1313/publication/gao2024mta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2024mta/","section":"publication","summary":"","tags":[],"title":"Toward an end-to-end implicit addressee modeling for dialogue disentanglement","type":"publication"},{"authors":[],"categories":[],"content":"","date":1713841200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1713841200,"objectID":"1e9261e53f298e1b55d67b1cf1d7467c","permalink":"http://localhost:1313/project/dialogue/","publishdate":"2024-04-23T11:00:00+08:00","relpermalink":"/project/dialogue/","section":"project","summary":"南京龙垣信息科技有限公司, 民口横向","tags":[],"title":"面向公安AI智能接警大模型自动问答系统","type":"project"},{"authors":["阮佳程","高景盛","谢铭烨","Daize Dong","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1712124005,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1712124005,"objectID":"ce75975d6deec56de732ba49fc3fd7aa","permalink":"http://localhost:1313/publication/ruan2024idat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2024idat/","section":"publication","summary":"","tags":[],"title":"iDAT: inverse Distillation Adapter-Tuning","type":"publication"},{"authors":["Zeyu Li","高景盛","向孙程","Zhuowei Wang","刘婷","付宇卓"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1711929600,"objectID":"3f9209a4994150a3efc5e3bcb9221d57","permalink":"http://localhost:1313/publication/li2024clapp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/li2024clapp/","section":"publication","summary":"","tags":[],"title":"Toward an end-to-end implicit addressee modeling for dialogue disentanglement","type":"publication"},{"authors":["谢铭烨","Zongwei Liu","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1709337600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1709337600,"objectID":"4e2f531e7e38b14231deeae36395699a","permalink":"http://localhost:1313/publication/xie2024mta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/xie2024mta/","section":"publication","summary":"","tags":[],"title":"Editing outdoor scenes with a large annotated synthetic dataset","type":"publication"},{"authors":["闫彬洁","徐琳","于泽芳","谢铭烨","冉苇","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1707976805,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1707976805,"objectID":"b86c082af6bd7bb9eb7afd8bebe0a6eb","permalink":"http://localhost:1313/publication/yan2024date/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/yan2024date/","section":"publication","summary":"","tags":[],"title":"Learning to Floorplan like Human Experts via Reinforcement Learning","type":"publication"},{"authors":["向孙程","Hao Chen","冉苇","于泽芳","刘婷","Dahong Qian","付宇卓"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1718064000,"objectID":"119ccca3419452f50cb09533661be3f9","permalink":"http://localhost:1313/publication/xiang-2024-deep/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/xiang-2024-deep/","section":"publication","summary":"Person re-identification plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. Recently, leveraging the supervised or semi-unsupervised learning paradigms, which benefits from the large-scale datasets and strong computing performance, has achieved a competitive performance on a specific target domain. However, when Re-ID models are directly deployed in a new domain without target samples, they always suffer from considerable performance degradation and poor domain generalization. To address this challenge, we propose a Deep Multimodal Representation Learning network to elaborate rich semantic knowledge for assisting in representation learning during the pre-training. Importantly, a multimodal representation learning strategy is introduced to translate the features of different modalities into the common space, which can significantly","tags":[],"title":"Deep multimodal representation learning for generalizable person re-identification","type":"publication"},{"authors":["向孙程","Dahong Qian","高景盛","Zirui Zhang","刘婷","付宇卓"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1715385600,"objectID":"81a124d4c62f667e2df8dc267f78dce6","permalink":"http://localhost:1313/publication/xiang-2024-rethink/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/xiang-2024-rethink/","section":"publication","summary":"Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that CNN-based pretraining on ImageNet has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, here we investigate semantic-based pretraining as another method to utilize additional textual data against ImageNet pretraining. Specifically, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, a pure semantic-based pretraining approach named VTBR is proposed to adopt dense captions to learn visual representations with fewer images. We train convolutional neural networks from scratch on the captions of FineGPR","tags":[],"title":"Rethinking person re-identification via semantic-based pretraining","type":"publication"},{"authors":["阮佳程","谢铭烨","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1688515200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1688515200,"objectID":"c6324560b2604d008319a81986a5de52","permalink":"http://localhost:1313/publication/ruan2023egeunet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2023egeunet/","section":"publication","summary":"","tags":[],"title":"Ege-unet: an efficient group enhanced unet for skin lesion segmentation","type":"publication"},{"authors":["高景盛","Yixin Lian","Ziyi Zhou","Baoyuan Wang","付宇卓"],"categories":[],"content":"","date":1685944805,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1685944805,"objectID":"8fe677910c35c8da0f82f5c4a458002d","permalink":"http://localhost:1313/publication/gao2023livechat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2023livechat/","section":"publication","summary":"Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale text-based social media data and large pre-trained models, there is no guarantee these agents could also perform well in fast-growing scenarios, such as live streaming, due to the bounded transferability of pretrained models and biased distributions of public datasets from Reddit and Weibo, etc. To improve the essential capability of responding and establish a benchmark in the live opendomain scenario, we introduce the LiveChat dataset, composed of 1.33 million real-life Chinese dialogues with almost 3800 average sessions across 351 personas and fine-grained profiles for each persona. LiveChat is automatically constructed by processing numerous live videos on the Internet and naturally falls within the scope of multi-party conversations, where the issues of Who says What to Whom should be considered. Therefore, we target two critical tasks of response modeling and addressee recognition and propose retrieval-based baselines grounded on advanced techniques. Experimental results have validated the positive effects of leveraging persona profiles and larger average sessions per persona. In addition, we also benchmark the transferability of advanced generation-based models on LiveChat and pose some future directions for current challenges.","tags":[],"title":"LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming","type":"publication"},{"authors":["Da Shi","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1677996005,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677996005,"objectID":"1320ccc435ad99e733bff76a82c18a9c","permalink":"http://localhost:1313/publication/shi2023dynaslim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/shi2023dynaslim/","section":"publication","summary":"","tags":[],"title":"DynaSlim: Dynamic Slimming for Vision Transformers","type":"publication"},{"authors":[],"categories":[],"content":"","date":1676948400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676948400,"objectID":"da3485b4fec44b8610e361c9ce3a1961","permalink":"http://localhost:1313/project/floorpan/","publishdate":"2023-02-21T11:00:00+08:00","relpermalink":"/project/floorpan/","section":"project","summary":"矽极软件技术（上海）有限公司, 民口横向","tags":[],"title":"基于人工智能的floorplan自动生成与量化评价算法","type":"project"},{"authors":["Yangcheng Li","于泽芳","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1676419200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676419200,"objectID":"cf9ba2ab6ce422077644c43625782e43","permalink":"http://localhost:1313/publication/li2023avtad/","publishdate":"2023-02-15T00:00:00Z","relpermalink":"/publication/li2023avtad/","section":"publication","summary":"","tags":[],"title":"AV-TAD: Audio-Visual Temporal Action Detection with Transformer","type":"publication"},{"authors":["于泽芳","胡燕萍","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1676419200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676419200,"objectID":"389dc8ac71179cefb3a323849c208d17","permalink":"http://localhost:1313/publication/yu2023ccposenet/","publishdate":"2023-02-15T00:00:00Z","relpermalink":"/publication/yu2023ccposenet/","section":"publication","summary":"","tags":[],"title":"CC-PoseNet: Towards human pose estimation in crowded classrooms","type":"publication"},{"authors":["阮佳程","向孙程","谢铭烨","刘婷","付宇卓"],"categories":null,"content":"","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1666224000,"objectID":"9a074d5c939900f5647d9faa0e2142ef","permalink":"http://localhost:1313/publication/ruan2022malunet/","publishdate":"2022-10-20T00:00:00Z","relpermalink":"/publication/ruan2022malunet/","section":"publication","summary":"Recently, some pioneering works have preferred applying more complex modules to improve segmentation performances. However, it is not friendly for actual clinical environments due to limited computing resources. To address this challenge, we propose a light-weight model to achieve competitive performances for skin lesion segmentation at the lowest cost of parameters and computational complexity so far. Briefly, we propose four modules: (1) DGA consists of dilated convolution and gated attention mechanisms to extract global and local feature information; (2) IEA, which is based on external attention to characterize the overall datasets and enhance the connection between samples; (3) CAB is composed of 1D convolution and fully connected layers to perform a global and local fusion of multi-stage features to generate attention maps at channel axes; (4) SAB, which operates on multi-stage features by a shared 2D convolution to generate attention maps at spatial axes. We combine four modules with our U-shape architecture and obtain a light-weight medical image segmentation model dubbed as MALUNet. Compared with UNet, our model improved the mIoU and DSC metrics by 2.39% and 1.49%, respectively, with a 44x and 166x reduction in the number of parameters and computational complexity. In addition, we conducted comparison experiments on two skin lesion segmentation datasets (ISIC2017 and ISIC2018). Experimental results show that our model achieves state-of-the-art in balancing the number of parameters, computational complexity and segmentation performances. Code is available at https://github.com/JCruan519/MALUNet.","tags":["Light-weight model","Medical image segmentation","Attention mechanism","Mobile health"],"title":"MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation","type":"publication"},{"authors":["谢铭烨","向孙程","Feng Wang","刘婷","付宇卓"],"categories":null,"content":"","date":1646524800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646524800,"objectID":"fc08f3ed02759b76a8a59d61b18836c1","permalink":"http://localhost:1313/publication/xie2022spatial/","publishdate":"2022-03-06T00:00:00Z","relpermalink":"/publication/xie2022spatial/","section":"publication","summary":"Facial attribute manipulation has attracted great attention from the public due to its wide range of applications. Aiming to smoothly manipulate the attributes of real facial images, it is critical to search a proper latent code aligns with the domain of pre-trained GAN for faithful inversion and control the transformation within the scope of the attribute for precise editing. Previous methods mainly focused on improving the quality of reconstruction, but often ignored the editing effect. To address this issue, we first propose a mapping network to manipulate latent code which is effective for diverse situations, and design a spatial attention network to predict binary mask of certain attribute which encourages to only alter relevant region of images and suppress irrelevant changes. In addition, we introduce a novel latent space into GAN inversion framework which achieves high reconstruction quality especially preserving identity features and retains ability to edit face attributes. Our methods pave the way to semantically meaningful and disentangled manipulations on both generated images and real images. Experimental results indicate a clear improvement over the current state-of-the-art methods both in subjective and objective metrics.","tags":["Generative adversarial network (GAN)","Facial attribute manipulation","Attention mechanism"],"title":"Spatial Attention Guided Local Facial Attribute Editing","type":"publication"},{"authors":[],"categories":null,"content":" 本科生课程《人工智能硬件综合实践课程》\n本科生毕业设计\n本科生PRP项目\n","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644796800,"objectID":"a82a3c00285a54111874ffaa7ee1cc4e","permalink":"http://localhost:1313/post/22-02-14-semester-task/","publishdate":"2022-02-14T00:00:00Z","relpermalink":"/post/22-02-14-semester-task/","section":"post","summary":"","tags":[],"title":"2022春 实验室任务规划","type":"post"},{"authors":[],"categories":null,"content":"列表待更新\u0026hellip;\n","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1643673600,"objectID":"e4c3c794c1877329dda2498565ebc54e","permalink":"http://localhost:1313/post/22-02-01-confence-list/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/post/22-02-01-confence-list/","section":"post","summary":"","tags":[],"title":"2022年上半年论文列表","type":"post"},{"authors":["谢铭烨","刘婷","付宇卓"],"categories":[],"content":"","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644989045,"objectID":"fd87377041c5d18ea988a39a11380001","permalink":"http://localhost:1313/publication/xie2022gos/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/publication/xie2022gos/","section":"publication","summary":"Scene editing has attracted increasing research interests owing to its valuable applications in the field of photography, entertainment. With style-based GAN being proposed, images can be reasonably edited on specific semantic by manipulating in latent space of generator. However, existing datasets cannot satisfy the demands of large amounts of diverse data and rich semantic annotations at the same time, which makes the existing method difficult to edit on the content of outdoor scene images. To address these problems, we propose a large-scale, diverse synthetic dataset called ``GOS dataset\" generated based on video game, which contains fine-grained semantic annotations. Extensive experiments show that utilizing the features obtained from the annotations of our dataset achieves better performance in outdoor scene editing, especially for distance and viewpoint of scenes, which indicates the extracted features have a certain generalization capability.","tags":[],"title":"GOS: A Large-Scale Annotated Outdoor Scene Synthetic Dataset","type":"publication"},{"authors":["于泽芳","Yangcheng Li","刘奕成","刘婷","付宇卓"],"categories":[],"content":"","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644992645,"objectID":"6edd947f836e81940b5d15f6ffb8475a","permalink":"http://localhost:1313/publication/yu2022synthetic/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/publication/yu2022synthetic/","section":"publication","summary":"Deep learning-based methods for human pose estimation require large volumes of training data to achieve superior performance. However, data acquisition in classroom environments raises privacy concerns, which will undoubtedly hinder the development of the latest deep learning techniques in education domain. Due to the absence of large, richly annotated classroom datasets, research into classroom observation has had to be done by manually collecting and annotating datasets. Unfortunately, the annotation of such data is time-consuming and challenging in over-crowded classrooms. To break through these limitations, we open source SynPose, a large, densely labeled synthetic dataset specifically designed for crowded human pose estimation in classroom and meeting scenarios. Moreover, we propose a novel CTGAN to bridge the domain gap. Comprehensive experiments on real-world classroom images show that our proposed dataset and method deliver important performance benefits compared to existing datasets, revealing the potential of SynPose for future studies.","tags":[],"title":"SynPose: A Large-scale and Densely Annotated Synthetic Dataset for Human Pose Estimation in Classroom","type":"publication"},{"authors":[],"categories":null,"content":"祝贺\n谢铭烨同学的论文\u0026quot;GOS: A Large-Scale Annotated Outdoor Scene Synthetic Dataset\u0026quot; 和\n于泽芳同学的论文\u0026quot;SynPose: A Large-Scale and Densely Annotated Synthtic Dataset for Human Pose Estimation in Classroom\u0026quot;\n被ICASSP 2022所接收！\nICASSP会议属于CCF B类会议。本次会议将于2022年5月在新加坡以混合模式召开。\n","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1642809600,"objectID":"d7c7131dcec88a807343a48d4e777e6d","permalink":"http://localhost:1313/post/22-01-22-paper-accepted/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/post/22-01-22-paper-accepted/","section":"post","summary":"CCF B类会议","tags":[],"title":"祝贺实验室两篇论文被ICASSP 2022接收","type":"post"},{"authors":[],"categories":[],"content":"","date":1642734000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1642734000,"objectID":"f2e649c015f39ba69163c26e5ae149c5","permalink":"http://localhost:1313/project/water/","publishdate":"2022-01-21T11:00:00+08:00","relpermalink":"/project/water/","section":"project","summary":"上海海洋装备前瞻技术研究院, 民口横向","tags":[],"title":"面向水声目标识别的深度学习网络快速在线学习方法","type":"project"},{"authors":["Jinjie Chen","付宇卓","Yibo Jin","刘婷"],"categories":[],"content":"","date":1638662400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638684005,"objectID":"05d0ede5ad44d8b6cf59e7ba41133317","permalink":"http://localhost:1313/publication/chen2021dffcn/","publishdate":"2021-12-05T00:00:00Z","relpermalink":"/publication/chen2021dffcn/","section":"publication","summary":"Recently, micro-expression recognition (MER) has attracted much attention due to its wide application in various fields such as crime trials and psychotherapy. However, the short duration and subtle movement of facial muscles make it difficult to extract micro-expression features. In this article, we propose a Dual Flow Fusion Convolutional Network (DFFCN) that combines the learning flow and optical flow to capture spatiotemporal features. Specifically, we adopt a trainable Learning Flow Module to extract the frame-level motion characteristics, fused with the mask generated from hand-crafted optical flow, and finally predict the micro-expression. Additionally, to overcome the shortcomings of limited and imbalanced training samples, we propose a data augmentation strategy based on Generative Adversarial Network (GAN). Comprehensive experiments are conducted on three public micro-expression datasets: CASME II, SAMM and SMIC with Leave-One-Subject-Out (LOSO) cross-validation. The results demonstrated that our method achieves competitive performance when compared with the existing approaches, with the best UF1 (0.8452) and UAR (0.8465).","tags":[],"title":"DFFCN: Dual Flow Fusion Convolutional Network for Micro Expression Recognition","type":"publication"},{"authors":["Yibo Jin","刘婷","Jinjie Chen","付宇卓"],"categories":[],"content":"","date":1638403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638424805,"objectID":"17649faa6880d5d012279c9f66b513ef","permalink":"http://localhost:1313/publication/jin2021dynamic/","publishdate":"2021-12-02T00:00:00Z","relpermalink":"/publication/jin2021dynamic/","section":"publication","summary":"Real-time object detection is a highly practical technical means which has been the focus of research in recent years. While the high requirement of running memory and computing resources hinder the deployment on resource-limited devices. In this paper, we propose an effective method to dynamically enhance the sparsity on channel-level. To this end, we introduce dynamic sparsity coefficient (DSC) to balance model training and sparse training as well as adaptable sparsity regularization (TLp) to reinforce sparsity. We monitor the saliency of channels to evaluate their significance for model performance during the sparse training process, according to which, we implement different sparsity strategy on channels. Aiming to maintain the representation ability of important parameters on a fine-grained level, we automatically discern insignificant channels and remove with channel pruning. We demonstrate our method on latest object detector YOLOv4 and lightweight model YOLOv4-Tiny. Compare with uncompressed model, our method can obtain 85.8% decrease of FLOPs, 88.9% declines of parameters with a moderate accuracy loss. Compared with other model compression methods, we achieves comparable results with fewer trainable parameters but better detection performance.","tags":[],"title":"Dynamic Channel Pruning for Real-Time Object Detection Networks","type":"publication"},{"authors":["Mengnan Qi","Hao Liu","付宇卓","刘婷"],"categories":[],"content":"","date":1638230400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638252005,"objectID":"eca5db69037fe9b000d1a67050243980","permalink":"http://localhost:1313/publication/qi2021improving/","publishdate":"2021-11-30T00:00:00Z","relpermalink":"/publication/qi2021improving/","section":"publication","summary":"With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI.","tags":[],"title":"Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment","type":"publication"},{"authors":[],"categories":null,"content":" AAAI: AAAI Conference on Artificial Intelligence CCF-A Deadline: 2021-08-30 Topic: Artificial Intelligence, Machine Learning, Applications Website: https://www.aaai.org/Conferences/AAAI-22/ ICLR: International Conference on Learning Representations Deadline: 2021-10-06 Topic: Artificial Intelligence, Representation Learning Website: https://iclr.cc/ ICASSP: IEEE International Conference on Acoustics, Speech, and Signal Processing CCF-B Deadline: 2021-10-01 Topic: Deep Learning, Machine Learning for Signal Processing Website: https://2022.ieeeicassp.org/ ECAI: European Conference on Artificial Intelligence CCF-B Deadline: 2021-11 (TBD) Topic: Artificial Intelligence, Machine Learning CVPR: IEEE Conference on Computer Vision and Pattern Recognition CCF-A Deadline: 2021-11 (TBD) Topic: Deep Learning and Computer Vision ICME: IEEE International Conference on Multimedia and Expo CCF-B Deadline: 2021-12 (TBD) Topic: Deep Learning and Multimedia ","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624492800,"objectID":"2d7d95da9ea56fb824eb674c3491e9ab","permalink":"http://localhost:1313/post/21-07-01-confence-list/","publishdate":"2021-06-24T00:00:00Z","relpermalink":"/post/21-07-01-confence-list/","section":"post","summary":"","tags":[],"title":"2021年下半年论文列表","type":"post"},{"authors":[],"categories":[],"content":"背景介绍 我国教育部于 2004 年启动“高等学校教学质量与教学改革工程”，在 2007年全面实施该工程，并启动了人才培养数据库及预测机制研究项目。十九大也进一步提出要实现高等教育内涵式发展，提升教育质量成为高等教育内涵式发展的第一要务。本项目从教育评价经典理论出发，探究了机器视觉和自然语言处理等领域最新人工智能技术在大学课堂学生参与行为的表征建模和分析报告生成等关键任务中的应用范式。\n应用场景和方案 项目以上海交通大学课堂真实视频为参考，构建了教室场景密集坐姿人群合成数据集 SynPose 和典型课堂参与行为关节点时序数据集，在此基础上设计了密集遮挡人群姿态估计模型 CC-PoseNet 和多模态时序动作检测模型 AV-TAD，进而提出了从原始课堂视频到教育学视角学生行为分析与反馈报告生成的端到端智能化系统，开创性地利用 GPT4 等大语言模型弥合了教育理论与视觉表征之间的认知层次差异，为教育学视角下的课堂行为自动化观察、 分析与评价反馈生成提供了一种高效可行的智能化实施方案。\n已有成果 项目执行期间共计发表/录用多篇CCF推荐会议/期刊论文和高水平教育类论文，其中主线任务所形成的结论性成果“端到端课堂行为分析框架”被EAAI2024接收，阶段性成果教室场景姿态估计数据集 SynPose、遮挡人群姿态估计模型 CC-PoseNet、“视频-音频”动作检测模型 AV-TAD发表于ICASSP2022/2023，辅助任务中多轮对话摘要模型EHMNet和多模态提示学习标签对齐LAMM分别发表/录用于EMNLP2022和AAAI2024。\n","date":1624244400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624244400,"objectID":"1a6865833907593b247fe726973dbf0b","permalink":"http://localhost:1313/project/education/","publishdate":"2021-06-21T11:00:00+08:00","relpermalink":"/project/education/","section":"project","summary":"国家自然科学基金委员会, 面上项目","tags":[],"title":"大学生课堂参与度的人工智能实时智慧评价关键技术研究","type":"project"},{"authors":[],"categories":[],"content":"背景介绍 尽管深度学习算法模型性能强大，然而由于其参数量多、运行所需要的算力极其庞大，使得深度学习网络难以在存储资源或计算资源受限的平台上部署运行，只能部署在GPU等硬件设备上进行加速计算。这样的特点大大限制了其应用场景，要求我们对深度学习网络模型进行优化和压缩，减少其对资源的需求，以便灵活部署到低资源的硬件平台中。\n应用场景 模型压缩应用于算力有限的边缘设备，比如车载平台，无人机等。在不影响准确性的前提下，制作能在边缘设备约束下运行的更小的模型具有广泛的应用价值。\n已有成果 设计了一种动态稀疏方法，该方案一方面加深对于模型冗余参数的挖掘力度，另一方面细粒度平衡稀疏训练和精度训练的制约关系。实现了更高剪枝率下的精度保持。\n不同压缩方式对比图 ","date":1624244400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624244400,"objectID":"064100558e303796b94cd8d6c19ebec2","permalink":"http://localhost:1313/project/compress/","publishdate":"2021-06-21T11:00:00+08:00","relpermalink":"/project/compress/","section":"project","summary":"上海汽车工业科技发展基金会, 民口横向","tags":[],"title":"深度学习压缩剪枝算法研究与应用","type":"project"},{"authors":["谢铭烨","feng-wang"],"categories":[],"content":"背景介绍 生成对抗网络（Generative Adversarial Networks， GAN）由两个基础神经网络即生成器（Generator）和判别器（Discriminator）所组成，其中一个用于生成内容，另一个则用于判别生成的内容。GAN能够学习数据集内的特征，进而生成逼近真实数据的高质量的图片。\nGAN在图像合成领域的应用已经十分广泛。近期的一些研究表明，在学习合成图像时，GAN 会自发地在隐空间（latent space）中表示出多种可解释属性，如用于人脸合成的性别特征、用于场景合成的光照条件。通过正确识别这些语义，我们可以将 GAN 学习到的知识重新利用，合理地控制图像生成过程，从而实现图像编辑功能的更广泛应用，如人脸操纵和场景编辑。\n解释 GAN 潜在空间的关键点在于找到与人类可理解属性相对应的子空间。通过这种方法，将潜码（latent code）向特定子空间的方向移动，即可对应地改变合成图像的语义。\n应用场景 人脸图像内含有很多的语义属性，例如表情、头发颜色、年龄等，利用GAN可以实现图像的属性编辑，比如改变表情、改变头发颜色。人脸属性编辑可应用在娱乐场景中，比如短视频中的特效，可实现年龄转换、表情变换等功能，也可用于辅助诸如人脸识别，表情识别等其他任务。\n已有成果 设计了一个针对人脸表情编辑的框架，该方案使得编辑后生成的图片质量更高、更真实。 人脸表情编辑效果 利用GTA V自动构建了带标注的室外场景数据集。 GTA V室外场景数据集 ","date":1624242900,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624242900,"objectID":"fbb157b9e65db8288337a67506523041","permalink":"http://localhost:1313/project/gan-editing/","publishdate":"2021-06-21T10:35:00+08:00","relpermalink":"/project/gan-editing/","section":"project","summary":"GAN-based image attribute editing","tags":[],"title":"基于GAN的图片属性编辑","type":"project"},{"authors":["向孙程","hao-chen","mengyuan-guan"],"categories":[],"content":"背景介绍 行人重识别（Person re-identification, Re-ID)，也称为行人再识别或跨镜追踪；主要实现从一个摄像头捕获的目标行人，到其他不同摄像头检索是否存在相同行人，即进行跨摄像头检索。\n应用场景 行人重识别技术可以弥补目前固定摄像头的视觉局限, 并可与行人检测、行人跟踪技术相结合, 应用于视频监控、智能安防等领域。\n已有成果 行人重识别领域由于安全隐私、数据可获取性、标注难度等因素的限制，高质量大规模的真实标注数据依然非常稀缺。针对这类问题，我们针对具体任务场景，利用GTA-5游戏引擎自动构建了多属性有标注数据集。 来自标注数据集的行人样本 已发表论文 [1] Xiang S, Fu Y, You G, et al. Unsupervised domain adaptation through synthesis for person re-identification[C]//2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2020: 1-6. Link\n[2] Xiang S, Fu Y, You G, et al. Taking A Closer Look at Synthesis: Fine-Grained Attribute Analysis for Person Re-Identification[C]//ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 3765-3769. Link\n","date":1624242600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624242600,"objectID":"cbfa17ff77cfd74b2a3732572f8f4f37","permalink":"http://localhost:1313/project/reid/","publishdate":"2021-06-21T10:30:00+08:00","relpermalink":"/project/reid/","section":"project","summary":"Person re-identification","tags":[],"title":"行人重识别","type":"project"},{"authors":["Feng Wang","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624256645,"objectID":"c6fd4297aa55ba5e4109ac5f5a9e7966","permalink":"http://localhost:1313/publication/wang2021attention/","publishdate":"2021-06-21T06:24:05.111767Z","relpermalink":"/publication/wang2021attention/","section":"publication","summary":"Facial expression manipulation has two objectives: 1) generating an image with target expression; 2) preserving the identity information of the original image as much as possible. Recently, Generative Adversarial Networks (GANs) have shown the abilities for fine-grained facial expression manipulation. However, current methods are still prone to generate images with poor quality. In this work, we propose a U-Net based generator with multi-attention gate for facial expression manipulation. The multi-level attention mechanism is helpful to manipulate relevant regions and preserve identity features, thus improving the editing ability. Furthermore, we adopt self-attention block to replace direct skip-connection to get long-range dependency in images. To suppress artifacts in generated images, we add a discriminator based loss function in the training process. Extensive experiments on both quantitative and qualitative evaluation show that our proposed method achieves better performance for facial expression manipulation.","tags":[],"title":"Attention Based Facial Expression Manipulation","type":"publication"},{"authors":[],"categories":null,"content":"上海交通大学微纳电子学系2022级 直硕/直博 研究生优才夏令营通知\n报名时间：即日起至2021年6月28日\n欢迎广大学子报考iCAT实验室！\n","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624233600,"objectID":"95d119f9fb8fd8878dd070f83cac9048","permalink":"http://localhost:1313/post/21-06-21-recruit/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/21-06-21-recruit/","section":"post","summary":"欢迎加入iCAT实验室！","tags":[],"title":"招收2022级硕士研究生","type":"post"},{"authors":[],"categories":null,"content":"庆祝iCAT主页重新启动！ 请实验室各位成员及时补充个人相关信息！\n","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616025600,"objectID":"1c229b80af11f553450fc9b355e0fa6a","permalink":"http://localhost:1313/post/21-03-18-homepage-start/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/post/21-03-18-homepage-start/","section":"post","summary":"http://sjtu-icat.github.io","tags":[],"title":"iCAT主页启动","type":"post"},{"authors":[],"categories":null,"content":"生成器：利用随机采样的隐空间编码生成真实度极高的合成图片，例如StyleGAN等\n编码器：将真实图片转换为隐空间编码，以便于后续的编辑和生成\n预测器：判断生成图片的语义信息，例如人物的性别、是否微笑等\n属性编辑：对隐空间编码进行特定向量方向的操控，以达到对图片属性编辑的目的\n","date":1615294800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1615294800,"objectID":"bd6b0e8ac5bcab56230631e93202b16a","permalink":"http://localhost:1313/event/21-03-09-phd-pre/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/21-03-09-phd-pre/","section":"event","summary":"StyleGAN及其变体介绍","tags":[],"title":"博士组会汇报","type":"event"},{"authors":[],"categories":null,"content":" International Conference on Multimodla Interaction (ICMI)\nMay 2021 CCF-C International Conference on Neural Information Processing (ICONIP)\nJune 2021 CCF-C International Conference on Tools with Artifical Intelligence (ICTAI)\nJuly 2021 CCF-C ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1614556800,"objectID":"64db1730dc1ad59c6df7d6912fb79440","permalink":"http://localhost:1313/post/21-03-01-confence-list/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/21-03-01-confence-list/","section":"post","summary":"请大家努力！","tags":[],"title":"2021年上半年论文列表","type":"post"},{"authors":[],"categories":null,"content":"祝贺向孙程同学的论文\u0026quot;Taking a Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification\u0026quot;被ICASSP'21所录用！\n该会议属于CCF B类会议。\n","date":1611792000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1611792000,"objectID":"7723946348269c992c6e67b992052eaa","permalink":"http://localhost:1313/post/20-12-02-paper-accepted/","publishdate":"2021-01-28T00:00:00Z","relpermalink":"/post/20-12-02-paper-accepted/","section":"post","summary":"CCF B类会议","tags":[],"title":"祝贺向孙程同学的论文被ICASSP'21录用","type":"post"},{"authors":["向孙程","付宇卓","尤冠杰","刘婷"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624256645,"objectID":"12ad3aa9f306b0cdffc7005003a7171a","permalink":"http://localhost:1313/publication/xiang-2021-taking/","publishdate":"2021-06-21T06:24:05.111767Z","relpermalink":"/publication/xiang-2021-taking/","section":"publication","summary":"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has achieved remarkable performance. However, in pursuit of high accuracy, researchers in the academic always focus on training with large-scale datasets at a high cost of time and label expenses, while neglect to explore the potential of performing efficient training from millions of synthetic data. To facilitate development in this field, we reviewed the previously developed synthetic dataset GPR and built an improved one (GPR+) with larger number of identities and distinguished attributes. Based on it, we quantitatively analyze the influence of dataset attribute on re-ID system. To our best knowledge, we are among the first attempts to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. This research helps us have a deeper understanding of the fundamental problems in person re-ID, which also provides useful insights for dataset building and future practical usage.","tags":[],"title":"Taking A Closer Look at Synthesis: Fine-Grained Attribute Analysis for Person Re-Identification","type":"publication"},{"authors":[],"categories":null,"content":"使用服务器时请自觉遵守使用说明！\n服务器种类 .158：Nvidia Tesla P100 SXM2 （四核，16GB） .249：Nvidia RTX 3090*4 （单核，24GB） 本地：Nvidia RTX 1080 Ti （单核，11GB） 载入服务器模块 每次登入服务器时均需要加载\n显示当前可用的软件清单 module av\n加载常用模块 module load anaconda3/2019.10 #亦可在自己路径下独立安装anaconda module load cuda/11.1 module load cudnn/8.0.5 module load gcc/7.3.0 conda简要使用说明 初始化conda shell conda init bash\n创建虚拟环境（以testenv举例） conda create -n testenv\n激活虚拟环境 conda activate testenv\n安装软件包 conda install numpy\nconda install pytorch=0.4.0 # 指定具体版本\n退出当前的虚拟环境 conda deactivate\n查看所有的虚拟环境 conda env list\n查看当前虚拟环境安装包 conda list\n删除虚拟环境及其下面的所有包 conda remove -n testenv —-all\n其他说明 软件最低版本需求 在3090上低于该版本将无法正常运行需要GPU的程序\nPyTorch\u0026gt;=1.7.0 Tensorflow\u0026gt;=2.4.0 服务器资源的分配 使用nvidia-smi或者gpustat(需使用pip或conda安装)查看当前服务器GPU使用状态 多数程序运行时会默认占用所有可用的GPU，需要长时间使用GPU时，在运行指令前添加CUDA_VISIBLE_DEVICES指定具体运行的GPU，例如CUDA_VISIBLE_DEVICES=0 python main.py即指定在0号GPU上运行程序 在服务器资源紧张时，将会根据任务优先级对资源进行协调（例如近期需要投会议或期刊的同学可以优先使用） 请注意自己home路径下的硬盘占用情况，定期清理临时文件 ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1609459200,"objectID":"b673e3c677f0f016bab2009c0b65feae","permalink":"http://localhost:1313/post/21-01-01-server-usage/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/post/21-01-01-server-usage/","section":"post","summary":"使用服务器时请自觉遵守使用说明！\n","tags":[],"title":"实验室服务器使用说明","type":"post"},{"authors":["向孙程","付宇卓","尤冠杰","刘婷"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616155521,"objectID":"75b5e41b3a8b2101fd8ed0fb990742b6","permalink":"http://localhost:1313/publication/xiang-2020-attribute/","publishdate":"2021-03-19T12:05:20.585767Z","relpermalink":"/publication/xiang-2020-attribute/","section":"publication","summary":"","tags":[],"title":"Attribute analysis with synthetic dataset for person re-identification","type":"publication"},{"authors":[],"categories":[],"content":"","date":1548039600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1548039600,"objectID":"0b0d14d88bf4c3886612263baf9f6849","permalink":"http://localhost:1313/project/intel/","publishdate":"2019-01-21T11:00:00+08:00","relpermalink":"/project/intel/","section":"project","summary":"英特尔（中国）有限公司, 民口横向","tags":[],"title":"基于Intel FPGA技术的合作项目开发","type":"project"},{"authors":["向孙程","付宇卓","刘婷"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616140988,"objectID":"9177e08ee4cd4a71d1d93c4a286ce6ac","permalink":"http://localhost:1313/publication/xiang-2019-deep/","publishdate":"2019-11-04T08:03:08.384351Z","relpermalink":"/publication/xiang-2019-deep/","section":"publication","summary":"The superiority of deeply learned representation has been reported in very recent literature of re-identification (Re-ID) task. In this paper, we study a novel transfer learning problem termed Distant Domain Transfer Learning (DDTL) for Re-ID task. Different from existing transfer learning problems which assume that there is a close relation between source domain and target domain, in the DDTL problem, target domain can be totally different from source domain. For example, the source domain classifies pedestrian images but the target domain distinguishes vehicle images. In this work, our goal is to execute an unseen and unrelated task based on a labeled dataset training previously without any samples from intermediate domains. Particularly, we consider the more pragmatic issue of learning a deep feature with no labels, and propose a Deep Unsupervised Progressive Learning (DUPL) method to transfer pretrained deep representations to unseen domains. Specifically, our work performs clustering and fine-tuning of the CNN to improve the performance of original model trained on the irrelevant labeled dataset. Empirical studies on distant domain adaptation task (pedestrian -\u003e vehicle) demonstrate the effectiveness of the proposed method, and the improvement in terms of the mAP accuracy is up to 15% over \"non-transfer\" methods.","tags":[],"title":"Deep unsupervised progressive learning for distant domain adaptation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://localhost:1313/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"http://localhost:1313/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"关于我们","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"http://localhost:1313/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"实验室项目","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"http://localhost:1313/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"团队成员","type":"widget_page"}]