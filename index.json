[{"authors":["yuzhuo-fu"],"categories":null,"content":"个人主页\n付宇卓，教授，博士生导师。\n博士毕业于哈尔滨工业大学计算机体系结构专业，现就职于上海交通大学电子信息与电气工程学院微纳电子系教授。\n1990年获得国防科技大学计算机系学士学位，1997年和2001分别获得哈尔滨工业大学计算机系统结构专业工学硕士和博士学位。2006年获教授职称，2010年获博导资格，华盛顿大学、康考迪亚大学访问学者、加拿大Concordia大学客座教授。\n发表国内外高水平论文100余篇，担任多个国际会议、核心期刊的审稿人。主持过国家863、上海市科委等课题，参与国家核、高、基重大科研项目，获得过2008年上海市领军人才荣誉，多次荣获上海交通大学优秀教师称号。\n担任国家集成电路人才培养基地委员会委员、上海交通大学电类学部学位委员会委员、上海交通大学国家集成电路人才培养基地负责人、上海交通大学教学指导委员会委员、上海交通大学实践教学委员会主任委员；上海集成电路行业协会副理事长单位上海交通大学负责人、上海市科委领域预测专家、上海市经信委聘任专家、闵行区科委专家、紫竹科技园集成电路平台建设专家。\nProf. Yuzhuo Fu\nVice director of student Innovation center, and leading the Innovative Computer Architecture and Technology Lab (iCAT) at Dept. of Micro/Nano Electronics, SJTU.\nHe received a B.S. degree from the Computer Engineering Department at Changsha Institute of Technology, and M.S. and Ph.D. degrees from the Computer Science and Engineering Department, Harbin Institute of Technology. Before joining SJTU in 2001, he worked as a Senior Engineer for the electric engineering institute of Heilongjiang University and Computing Center of Heilongjiang Province.\nHe is a Present Deputy Professor of the undergraduate student affairs office of SJTU.\nHis research interests include fault-tolerant architecture, heterogeneous system architecture, and edge computing architecture. Especially focused on application-driven design/architecture innovations, which include novel architectures for artificial intelligence (AI) and acceleration with CPU/GPU/FPGA.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"f02af2e35003a4fce8419938885e8c32","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"个人主页 付宇卓，教授，博士生导师。 博士毕业于哈尔滨工业大学计","tags":null,"title":"付宇卓","type":"authors"},{"authors":["ting-liu"],"categories":null,"content":"个人主页\n上海交通大学微电子学院软件工程硕士。曾参与微电子火工品系统控制总线协议设计和多核片上系统仿真平台的FPGA验证等项目的研发。\n主要研究方向包括基于FPGA的多核片上系统架构设计，多核架构下的系统功耗模型分析，片上网络架构及协议设计与验证等。\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"c26560187b7aae67ede0cbb7b3f6d1f5","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"个人主页 上海交通大学微电子学院软件工程硕士。曾参与微电子火工","tags":null,"title":"刘婷","type":"authors"},{"authors":["suncheng-xiang"],"categories":null,"content":"Hi, I am Suncheng Xiang, a final-year PhD student at Shanghai Jiao Tong University.\nBefore this, I graduated from Changsha University of Science \u0026amp; Technology (CSUST) for B.Eng in Electrical Engineering and Automation in 2014, then obtained the M.Eng from National University of Defense Technology (NUDT) in 2017.\nMy main research topics are computer vision and machine learning. Specifically, I am currently doing research works on person re-identification, domain adaptation, transfer learning and related applications. I am also interested in some image generation and image retrieval tasks, e.g. vehicle re-identification and fine-grained image retrieval tasks.\nThis is my Homepage.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"91425658346a37c7627888b5a2c7c0f6","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Hi, I am Suncheng Xiang, a final-year PhD student at Shanghai Jiao Tong University. Before this, I graduated from Changsha University of Science \u0026amp; Technology (CSUST) for B.Eng in Electrical","tags":null,"title":"向孙程","type":"authors"},{"authors":["zefang-yu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"8d85dd6b83d65856fb3e0add1bc61a4d","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"于泽芳","type":"authors"},{"authors":["mingye-xie"],"categories":null,"content":"I have received the B. Sc. degree from East China Normal University, China, in 2016, the M. Eng. Degree in electronics and communications engineering from East China Normal University, China, in 2019.\nI am currently pursing Ph. D. degree in computer science and technology from Shanghai Jiao Tong University, China.\nMy research interests include Generative Adversial Network (GAN) and model compressing.\nFurther information can check my Homepage.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"8913f79c0868d79a409c8ff959e200e9","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I have received the B. Sc. degree from East China Normal University, China, in 2016, the M. Eng. Degree in electronics and communications engineering from East China Normal University, China,","tags":null,"title":"谢铭烨","type":"authors"},{"authors":["wei-ran"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1718064000,"objectID":"a9790e59ba40e33e74534081e26117da","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"冉苇","type":"authors"},{"authors":["jingsheng-gao"],"categories":null,"content":"​ I am a first-year PhD student at SJTU EECS, advised by Prof. Yuzhuo Fu. Previously, I obtained my bachelor\u0026rsquo;s degree from school of Physics and Astromony, Shanghai Jiaotong University.\n​ I completed my undergraduate graduation project using different kinds of U-nets, which is a astronomical task called extracting weak gravitational lens in CMB. Also, I have ever practiced in a medical company which is established by Fosun. During my first year, I have tried some projects on document understanding, like seal extraction and tabel detection.\n​ In the future, I will mainly put my attention on computer version and try to catch the future development of machine learning.\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"6f881165b4c0103ea74df6299fd980f0","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"​ I am a first-year PhD student at SJTU EECS, advised by Prof. Yuzhuo Fu. Previously, I obtained my bachelor\u0026rsquo;s degree from school of Physics and Astromony, Shanghai Jiaotong University.","tags":null,"title":"高景盛","type":"authors"},{"authors":[null],"categories":null,"content":"None\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"4036ab9376e924db20b9e42fb1307811","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"None","tags":null,"title":"","type":"authors"},{"authors":["binjie-yan"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1707976805,"objectID":"12faef7d774d27c48ef33b87daab73ac","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"闫彬洁","type":"authors"},{"authors":["jiacheng-ruan"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1733356800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1733356800,"objectID":"c02afb82bb3ace28aade6f87659e605d","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"阮佳程","type":"authors"},{"authors":["lining-hu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"477d2ae37505dcfc23372cf0e25d606a","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"胡立宁","type":"authors"},{"authors":["lin-xu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1707976805,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1707976805,"objectID":"13bffac5dfb607ac9f0fd041fac5882b","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"徐琳","type":"authors"},{"authors":["xian-gao"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"f734440d7af507705fa9502d96445625","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"高显","type":"authors"},{"authors":["lin-xu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"671e5b184349c86b2bb0dd289e261027","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"袁文镇","type":"authors"},{"authors":["zongyu-zhang"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"1f4cf8d686ee0ce359e794d5856484d7","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"张宗云","type":"authors"},{"authors":["yanping-hu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":1676419200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1676419200,"objectID":"d5b4aa2c078fe36926f2c874bcdfaf38","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"胡燕萍","type":"authors"},{"authors":["admin"],"categories":null,"content":"网站管理员。\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"网站管理员。","tags":null,"title":"iCAT","type":"authors"},{"authors":["tong-yu"],"categories":null,"content":"Need to fill the bio\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"fadaa8758bda1bb2c1a5a13190f4c1f3","permalink":"http://localhost:1313/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Need to fill the bio\u0026hellip;","tags":null,"title":"于潼","type":"authors"},{"authors":["阮佳程","高景盛","谢铭烨","向孙程","于泽芳","刘婷","付宇卓"],"categories":[],"content":"","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1733356800,"objectID":"241508c8fa7c7890a40b0a69939dc6ca","permalink":"http://localhost:1313/publication/ruan2024gist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2024gist/","section":"publication","summary":"","tags":[],"title":"GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction","type":"publication"},{"authors":["于泽芳","谢铭烨","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1722988800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722988800,"objectID":"1184d4360b56297e4dc9c1911756ea1b","permalink":"http://localhost:1313/publication/yu2024raw/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/yu2024raw/","section":"publication","summary":"","tags":[],"title":"From Raw Video to Pedagogical Insights: A Unified Framework for Student Behavior Analysis","type":"publication"},{"authors":["高景盛","阮佳程","向孙程","于泽芳","谢铭烨","刘婷","付宇卓"],"categories":[],"content":"","date":1722902400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722902400,"objectID":"aaabef15de319334f2abe21214e1e5cc","permalink":"http://localhost:1313/publication/gao2024lamm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2024lamm/","section":"publication","summary":"","tags":[],"title":"LAMM: Label Alignment for Multi-Modal Prompt Learning","type":"publication"},{"authors":["高景盛","Zeyu Li","向孙程","Zhuowei Wang","刘婷","付宇卓"],"categories":[],"content":"","date":1722902400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1722902400,"objectID":"4b6e62694022f5873d46788f26d89c1d","permalink":"http://localhost:1313/publication/gao2024mta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2024mta/","section":"publication","summary":"","tags":[],"title":"Toward an end-to-end implicit addressee modeling for dialogue disentanglement","type":"publication"},{"authors":["阮佳程","高景盛","谢铭烨","Daize Dong","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1712124005,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1712124005,"objectID":"ce75975d6deec56de732ba49fc3fd7aa","permalink":"http://localhost:1313/publication/ruan2024idat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2024idat/","section":"publication","summary":"","tags":[],"title":"iDAT: inverse Distillation Adapter-Tuning","type":"publication"},{"authors":["高景盛","Zeyu Li","向孙程","Zhuowei Wang","刘婷","付宇卓"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1711929600,"objectID":"3f9209a4994150a3efc5e3bcb9221d57","permalink":"http://localhost:1313/publication/li2024clapp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/li2024clapp/","section":"publication","summary":"","tags":[],"title":"Toward an end-to-end implicit addressee modeling for dialogue disentanglement","type":"publication"},{"authors":["谢铭烨","Zongwei Liu","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1709337600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1709337600,"objectID":"4e2f531e7e38b14231deeae36395699a","permalink":"http://localhost:1313/publication/xie2024mta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/xie2024mta/","section":"publication","summary":"","tags":[],"title":"Editing outdoor scenes with a large annotated synthetic dataset","type":"publication"},{"authors":["闫彬洁","徐琳","于泽芳","谢铭烨","冉苇","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1707976805,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1707976805,"objectID":"b86c082af6bd7bb9eb7afd8bebe0a6eb","permalink":"http://localhost:1313/publication/yan2024date/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/yan2024date/","section":"publication","summary":"","tags":[],"title":"Learning to Floorplan like Human Experts via Reinforcement Learning","type":"publication"},{"authors":["向孙程","Hao Chen","冉苇","于泽芳","刘婷","Dahong Qian","付宇卓"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1718064000,"objectID":"81a124d4c62f667e2df8dc267f78dce6","permalink":"http://localhost:1313/publication/xiang-2024-rethink/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/xiang-2024-rethink/","section":"publication","summary":"Person re-identification plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. Recently, leveraging the supervised or semi-unsupervised learning paradigms, which benefits from the large-scale datasets and strong computing performance, has achieved a competitive performance on a specific target domain. However, when Re-ID models are directly deployed in a new domain without target samples, they always suffer from considerable performance degradation and poor domain generalization. To address this challenge, we propose a Deep Multimodal Representation Learning network to elaborate rich semantic knowledge for assisting in representation learning during the pre-training. Importantly, a multimodal representation learning strategy is introduced to translate the features of different modalities into the common space, which can significantly","tags":[],"title":"Deep multimodal representation learning for generalizable person re-identification","type":"publication"},{"authors":["向孙程","Dahong Qian","高景盛","Zirui Zhang","刘婷","付宇卓"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1715385600,"objectID":"119ccca3419452f50cb09533661be3f9","permalink":"http://localhost:1313/publication/xiang-2024-deep/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/xiang-2024-deep/","section":"publication","summary":"Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that CNN-based pretraining on ImageNet has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, here we investigate semantic-based pretraining as another method to utilize additional textual data against ImageNet pretraining. Specifically, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, a pure semantic-based pretraining approach named VTBR is proposed to adopt dense captions to learn visual representations with fewer images. We train convolutional neural networks from scratch on the captions of FineGPR","tags":[],"title":"Rethinking person re-identification via semantic-based pretraining","type":"publication"},{"authors":["阮佳程","谢铭烨","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1688515200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1688515200,"objectID":"c6324560b2604d008319a81986a5de52","permalink":"http://localhost:1313/publication/ruan2023egeunet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ruan2023egeunet/","section":"publication","summary":"","tags":[],"title":"Ege-unet: an efficient group enhanced unet for skin lesion segmentation","type":"publication"},{"authors":["高景盛","Yixin Lian","Ziyi Zhou","Baoyuan Wang","付宇卓"],"categories":[],"content":"","date":1685944805,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1685944805,"objectID":"8fe677910c35c8da0f82f5c4a458002d","permalink":"http://localhost:1313/publication/gao2023livechat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gao2023livechat/","section":"publication","summary":"","tags":[],"title":"LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming","type":"publication"},{"authors":["Da Shi","高景盛","刘婷","付宇卓"],"categories":[],"content":"","date":1677996005,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1677996005,"objectID":"1320ccc435ad99e733bff76a82c18a9c","permalink":"http://localhost:1313/publication/shi2023dynaslim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/shi2023dynaslim/","section":"publication","summary":"","tags":[],"title":"DynaSlim: Dynamic Slimming for Vision Transformers","type":"publication"},{"authors":["Yangcheng Li","于泽芳","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1676419200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676419200,"objectID":"cf9ba2ab6ce422077644c43625782e43","permalink":"http://localhost:1313/publication/li2023avtad/","publishdate":"2023-02-15T00:00:00Z","relpermalink":"/publication/li2023avtad/","section":"publication","summary":"","tags":[],"title":"AV-TAD: Audio-Visual Temporal Action Detection with Transformer","type":"publication"},{"authors":["于泽芳","胡燕萍","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1676419200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1676419200,"objectID":"389dc8ac71179cefb3a323849c208d17","permalink":"http://localhost:1313/publication/yu2023ccposenet/","publishdate":"2023-02-15T00:00:00Z","relpermalink":"/publication/yu2023ccposenet/","section":"publication","summary":"","tags":[],"title":"CC-PoseNet: Towards human pose estimation in crowded classrooms","type":"publication"},{"authors":["阮佳程","向孙程","谢铭烨","刘婷","付宇卓"],"categories":null,"content":"","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1666224000,"objectID":"9a074d5c939900f5647d9faa0e2142ef","permalink":"http://localhost:1313/publication/ruan2022malunet/","publishdate":"2022-10-20T00:00:00Z","relpermalink":"/publication/ruan2022malunet/","section":"publication","summary":"Recently, some pioneering works have preferred applying more complex modules to improve segmentation performances. However, it is not friendly for actual clinical environments due to limited computing resources. To address this challenge, we propose a light-weight model to achieve competitive performances for skin lesion segmentation at the lowest cost of parameters and computational complexity so far. Briefly, we propose four modules: (1) DGA consists of dilated convolution and gated attention mechanisms to extract global and local feature information; (2) IEA, which is based on external attention to characterize the overall datasets and enhance the connection between samples; (3) CAB is composed of 1D convolution and fully connected layers to perform a global and local fusion of multi-stage features to generate attention maps at channel axes; (4) SAB, which operates on multi-stage features by a shared 2D convolution to generate attention maps at spatial axes. We combine four modules with our U-shape architecture and obtain a light-weight medical image segmentation model dubbed as MALUNet. Compared with UNet, our model improved the mIoU and DSC metrics by 2.39% and 1.49%, respectively, with a 44x and 166x reduction in the number of parameters and computational complexity. In addition, we conducted comparison experiments on two skin lesion segmentation datasets (ISIC2017 and ISIC2018). Experimental results show that our model achieves state-of-the-art in balancing the number of parameters, computational complexity and segmentation performances. Code is available at https://github.com/JCruan519/MALUNet.","tags":["Light-weight model","Medical image segmentation","Attention mechanism","Mobile health"],"title":"MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation","type":"publication"},{"authors":["谢铭烨","向孙程","Feng Wang","刘婷","付宇卓"],"categories":null,"content":"","date":1646524800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1646524800,"objectID":"fc08f3ed02759b76a8a59d61b18836c1","permalink":"http://localhost:1313/publication/xie2022spatial/","publishdate":"2022-03-06T00:00:00Z","relpermalink":"/publication/xie2022spatial/","section":"publication","summary":"Facial attribute manipulation has attracted great attention from the public due to its wide range of applications. Aiming to smoothly manipulate the attributes of real facial images, it is critical to search a proper latent code aligns with the domain of pre-trained GAN for faithful inversion and control the transformation within the scope of the attribute for precise editing. Previous methods mainly focused on improving the quality of reconstruction, but often ignored the editing effect. To address this issue, we first propose a mapping network to manipulate latent code which is effective for diverse situations, and design a spatial attention network to predict binary mask of certain attribute which encourages to only alter relevant region of images and suppress irrelevant changes. In addition, we introduce a novel latent space into GAN inversion framework which achieves high reconstruction quality especially preserving identity features and retains ability to edit face attributes. Our methods pave the way to semantically meaningful and disentangled manipulations on both generated images and real images. Experimental results indicate a clear improvement over the current state-of-the-art methods both in subjective and objective metrics.","tags":["Generative adversarial network (GAN)","Facial attribute manipulation","Attention mechanism"],"title":"Spatial Attention Guided Local Facial Attribute Editing","type":"publication"},{"authors":[],"categories":null,"content":" 本科生课程《人工智能硬件综合实践课程》\n本科生毕业设计\n本科生PRP项目\n","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644796800,"objectID":"a82a3c00285a54111874ffaa7ee1cc4e","permalink":"http://localhost:1313/post/22-02-14-semester-task/","publishdate":"2022-02-14T00:00:00Z","relpermalink":"/post/22-02-14-semester-task/","section":"post","summary":"","tags":[],"title":"2022春 实验室任务规划","type":"post"},{"authors":[],"categories":null,"content":"列表待更新\u0026hellip;\n","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1643673600,"objectID":"e4c3c794c1877329dda2498565ebc54e","permalink":"http://localhost:1313/post/22-02-01-confence-list/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/post/22-02-01-confence-list/","section":"post","summary":"","tags":[],"title":"2022年上半年论文列表","type":"post"},{"authors":["谢铭烨","刘婷","付宇卓"],"categories":[],"content":"","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644989045,"objectID":"fd87377041c5d18ea988a39a11380001","permalink":"http://localhost:1313/publication/xie2022gos/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/publication/xie2022gos/","section":"publication","summary":"Scene editing has attracted increasing research interests owing to its valuable applications in the field of photography, entertainment. With style-based GAN being proposed, images can be reasonably edited on specific semantic by manipulating in latent space of generator. However, existing datasets cannot satisfy the demands of large amounts of diverse data and rich semantic annotations at the same time, which makes the existing method difficult to edit on the content of outdoor scene images. To address these problems, we propose a large-scale, diverse synthetic dataset called ``GOS dataset\" generated based on video game, which contains fine-grained semantic annotations. Extensive experiments show that utilizing the features obtained from the annotations of our dataset achieves better performance in outdoor scene editing, especially for distance and viewpoint of scenes, which indicates the extracted features have a certain generalization capability.","tags":[],"title":"GOS: A Large-Scale Annotated Outdoor Scene Synthetic Dataset","type":"publication"},{"authors":["于泽芳","Yangcheng Li","刘奕成","刘婷","付宇卓"],"categories":[],"content":"","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1644992645,"objectID":"6edd947f836e81940b5d15f6ffb8475a","permalink":"http://localhost:1313/publication/yu2022synthetic/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/publication/yu2022synthetic/","section":"publication","summary":"Deep learning-based methods for human pose estimation require large volumes of training data to achieve superior performance. However, data acquisition in classroom environments raises privacy concerns, which will undoubtedly hinder the development of the latest deep learning techniques in education domain. Due to the absence of large, richly annotated classroom datasets, research into classroom observation has had to be done by manually collecting and annotating datasets. Unfortunately, the annotation of such data is time-consuming and challenging in over-crowded classrooms. To break through these limitations, we open source SynPose, a large, densely labeled synthetic dataset specifically designed for crowded human pose estimation in classroom and meeting scenarios. Moreover, we propose a novel CTGAN to bridge the domain gap. Comprehensive experiments on real-world classroom images show that our proposed dataset and method deliver important performance benefits compared to existing datasets, revealing the potential of SynPose for future studies.","tags":[],"title":"SynPose: A Large-scale and Densely Annotated Synthetic Dataset for Human Pose Estimation in Classroom","type":"publication"},{"authors":[],"categories":null,"content":"祝贺\n谢铭烨同学的论文\u0026quot;GOS: A Large-Scale Annotated Outdoor Scene Synthetic Dataset\u0026quot; 和\n于泽芳同学的论文\u0026quot;SynPose: A Large-Scale and Densely Annotated Synthtic Dataset for Human Pose Estimation in Classroom\u0026quot;\n被ICASSP 2022所接收！\nICASSP会议属于CCF B类会议。本次会议将于2022年5月在新加坡以混合模式召开。\n","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1642809600,"objectID":"d7c7131dcec88a807343a48d4e777e6d","permalink":"http://localhost:1313/post/22-01-22-paper-accepted/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/post/22-01-22-paper-accepted/","section":"post","summary":"CCF B类会议","tags":[],"title":"祝贺实验室两篇论文被ICASSP 2022接收","type":"post"},{"authors":["Jinjie Chen","付宇卓","Yibo Jin","刘婷"],"categories":[],"content":"","date":1638662400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638684005,"objectID":"05d0ede5ad44d8b6cf59e7ba41133317","permalink":"http://localhost:1313/publication/chen2021dffcn/","publishdate":"2021-12-05T00:00:00Z","relpermalink":"/publication/chen2021dffcn/","section":"publication","summary":"Recently, micro-expression recognition (MER) has attracted much attention due to its wide application in various fields such as crime trials and psychotherapy. However, the short duration and subtle movement of facial muscles make it difficult to extract micro-expression features. In this article, we propose a Dual Flow Fusion Convolutional Network (DFFCN) that combines the learning flow and optical flow to capture spatiotemporal features. Specifically, we adopt a trainable Learning Flow Module to extract the frame-level motion characteristics, fused with the mask generated from hand-crafted optical flow, and finally predict the micro-expression. Additionally, to overcome the shortcomings of limited and imbalanced training samples, we propose a data augmentation strategy based on Generative Adversarial Network (GAN). Comprehensive experiments are conducted on three public micro-expression datasets: CASME II, SAMM and SMIC with Leave-One-Subject-Out (LOSO) cross-validation. The results demonstrated that our method achieves competitive performance when compared with the existing approaches, with the best UF1 (0.8452) and UAR (0.8465).","tags":[],"title":"DFFCN: Dual Flow Fusion Convolutional Network for Micro Expression Recognition","type":"publication"},{"authors":["Yibo Jin","刘婷","Jinjie Chen","付宇卓"],"categories":[],"content":"","date":1638403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638424805,"objectID":"17649faa6880d5d012279c9f66b513ef","permalink":"http://localhost:1313/publication/jin2021dynamic/","publishdate":"2021-12-02T00:00:00Z","relpermalink":"/publication/jin2021dynamic/","section":"publication","summary":"Real-time object detection is a highly practical technical means which has been the focus of research in recent years. While the high requirement of running memory and computing resources hinder the deployment on resource-limited devices. In this paper, we propose an effective method to dynamically enhance the sparsity on channel-level. To this end, we introduce dynamic sparsity coefficient (DSC) to balance model training and sparse training as well as adaptable sparsity regularization (TLp) to reinforce sparsity. We monitor the saliency of channels to evaluate their significance for model performance during the sparse training process, according to which, we implement different sparsity strategy on channels. Aiming to maintain the representation ability of important parameters on a fine-grained level, we automatically discern insignificant channels and remove with channel pruning. We demonstrate our method on latest object detector YOLOv4 and lightweight model YOLOv4-Tiny. Compare with uncompressed model, our method can obtain 85.8% decrease of FLOPs, 88.9% declines of parameters with a moderate accuracy loss. Compared with other model compression methods, we achieves comparable results with fewer trainable parameters but better detection performance.","tags":[],"title":"Dynamic Channel Pruning for Real-Time Object Detection Networks","type":"publication"},{"authors":["Mengnan Qi","Hao Liu","付宇卓","刘婷"],"categories":[],"content":"","date":1638230400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1638252005,"objectID":"eca5db69037fe9b000d1a67050243980","permalink":"http://localhost:1313/publication/qi2021improving/","publishdate":"2021-11-30T00:00:00Z","relpermalink":"/publication/qi2021improving/","section":"publication","summary":"With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI.","tags":[],"title":"Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment","type":"publication"},{"authors":[],"categories":null,"content":" AAAI: AAAI Conference on Artificial Intelligence CCF-A Deadline: 2021-08-30 Topic: Artificial Intelligence, Machine Learning, Applications Website: https://www.aaai.org/Conferences/AAAI-22/ ICLR: International Conference on Learning Representations Deadline: 2021-10-06 Topic: Artificial Intelligence, Representation Learning Website: https://iclr.cc/ ICASSP: IEEE International Conference on Acoustics, Speech, and Signal Processing CCF-B Deadline: 2021-10-01 Topic: Deep Learning, Machine Learning for Signal Processing Website: https://2022.ieeeicassp.org/ ECAI: European Conference on Artificial Intelligence CCF-B Deadline: 2021-11 (TBD) Topic: Artificial Intelligence, Machine Learning CVPR: IEEE Conference on Computer Vision and Pattern Recognition CCF-A Deadline: 2021-11 (TBD) Topic: Deep Learning and Computer Vision ICME: IEEE International Conference on Multimedia and Expo CCF-B Deadline: 2021-12 (TBD) Topic: Deep Learning and Multimedia ","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624492800,"objectID":"2d7d95da9ea56fb824eb674c3491e9ab","permalink":"http://localhost:1313/post/21-07-01-confence-list/","publishdate":"2021-06-24T00:00:00Z","relpermalink":"/post/21-07-01-confence-list/","section":"post","summary":"","tags":[],"title":"2021年下半年论文列表","type":"post"},{"authors":["yibo-jin","da-shi"],"categories":[],"content":"背景介绍 尽管深度学习算法模型性能强大，然而由于其参数量多、运行所需要的算力极其庞大，使得深度学习网络难以在存储资源或计算资源受限的平台上部署运行，只能部署在GPU等硬件设备上进行加速计算。这样的特点大大限制了其应用场景，要求我们对深度学习网络模型进行优化和压缩，减少其对资源的需求，以便灵活部署到低资源的硬件平台中。\n应用场景 模型压缩应用于算力有限的边缘设备，比如车载平台，无人机等。在不影响准确性的前提下，制作能在边缘设备约束下运行的更小的模型具有广泛的应用价值。\n已有成果 设计了一种动态稀疏方法，该方案一方面加深对于模型冗余参数的挖掘力度，另一方面细粒度平衡稀疏训练和精度训练的制约关系。实现了更高剪枝率下的精度保持。\n不同压缩方式对比图 ","date":1624244400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624244400,"objectID":"064100558e303796b94cd8d6c19ebec2","permalink":"http://localhost:1313/project/compress/","publishdate":"2021-06-21T11:00:00+08:00","relpermalink":"/project/compress/","section":"project","summary":"Model compression","tags":[],"title":"模型压缩","type":"project"},{"authors":["jinjie-chen"],"categories":[],"content":"背景介绍 微表情(Micro-expression)是人们在试图隐藏/压抑真实情感时出现的细微的面部表情。 不同于宏表情，微表情变化细微，动作幅度小，难以观察和辨别；而且发生时间短暂，持续时间不超过0.5s；微表情人们在试图隐藏/压抑真实情感时出现，是自发的，无意识的，可以看作是人的真实情感的“泄露”。\n应用场景 因此，微表情识别(MER)研究使人们对微妙的面部动作有了更强的意识和敏感性，是理解人类情绪和情感表达的重要课题，已被心理学、社会学、神经科学、计算机视觉等多个学科所探索。广泛应用于警察询问、临床诊断、抑郁症分析、商务谈判等领域\n已有成果 使用对抗生成网络生成微表情伪样本，从根本上解决样本多样性问题，在MEGC2019冠军模型上可以提升10%精度； 基于2D CNN的backbone，以视频作为输入，能同时学习时空域上的特征，形成端到端的训练，在CASMEII数据集上达到92%的F1； ","date":1624243800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624243800,"objectID":"51c1bbd7d153b0879a148279d270c469","permalink":"http://localhost:1313/project/micro-expr/","publishdate":"2021-06-21T10:50:00+08:00","relpermalink":"/project/micro-expr/","section":"project","summary":"Micro-expression recognition","tags":[],"title":"微表情识别","type":"project"},{"authors":["mengnan-qi","hao-liu"],"categories":[],"content":"背景介绍 会议摘要(meeting summarization),属于多人长对话场景下的文本摘要；就是在会议记录基础上经过加工、整理出来的一种记叙性和介绍性的文件。包括会议的基本情况、主要精神及中心内容。\n摘要内容示例 应用场景 实时自动会议纪要将大大简化繁琐的会议总结工作，可以帮助与会者快速获取会议重要信息，从而做出正确的判断。\n已有成果 相比于新闻等结构化的文本，会议文本结构复杂，不同角色的发言相互交错，主题杂糅，语义逻辑破碎，理解其文本语义是对现有深度学习模型的全新挑战。在这项工作中，我们提出了一个具有多任务预训练的分层变压器编解码网络。具体地说，我们在词级编码器上为每个话语屏蔽关键句，并在解码器上生成它们。此外，我们在输入文本中随机屏蔽一些角色对齐，并强制模型恢复原始的角色标签来完成对齐。此外，我们还引入了主题分割机制来约束关注范围，进一步提高了生成摘要的质量。实验结果表明，该模型在满足AMI和ICSI的汇总数据集上优于以往的方法。\n","date":1624242900,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624242900,"objectID":"fac891e21b3acf27b07f9a8cb9254e17","permalink":"http://localhost:1313/project/summary/","publishdate":"2021-06-21T10:35:00+08:00","relpermalink":"/project/summary/","section":"project","summary":"Meeting summarization","tags":[],"title":"会议摘要","type":"project"},{"authors":["谢铭烨","feng-wang"],"categories":[],"content":"背景介绍 生成对抗网络（Generative Adversarial Networks， GAN）由两个基础神经网络即生成器（Generator）和判别器（Discriminator）所组成，其中一个用于生成内容，另一个则用于判别生成的内容。GAN能够学习数据集内的特征，进而生成逼近真实数据的高质量的图片。\nGAN在图像合成领域的应用已经十分广泛。近期的一些研究表明，在学习合成图像时，GAN 会自发地在隐空间（latent space）中表示出多种可解释属性，如用于人脸合成的性别特征、用于场景合成的光照条件。通过正确识别这些语义，我们可以将 GAN 学习到的知识重新利用，合理地控制图像生成过程，从而实现图像编辑功能的更广泛应用，如人脸操纵和场景编辑。\n解释 GAN 潜在空间的关键点在于找到与人类可理解属性相对应的子空间。通过这种方法，将潜码（latent code）向特定子空间的方向移动，即可对应地改变合成图像的语义。\n应用场景 人脸图像内含有很多的语义属性，例如表情、头发颜色、年龄等，利用GAN可以实现图像的属性编辑，比如改变表情、改变头发颜色。人脸属性编辑可应用在娱乐场景中，比如短视频中的特效，可实现年龄转换、表情变换等功能，也可用于辅助诸如人脸识别，表情识别等其他任务。\n已有成果 设计了一个针对人脸表情编辑的框架，该方案使得编辑后生成的图片质量更高、更真实。 人脸表情编辑效果 利用GTA V自动构建了带标注的室外场景数据集。 GTA V室外场景数据集 ","date":1624242900,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624242900,"objectID":"fbb157b9e65db8288337a67506523041","permalink":"http://localhost:1313/project/gan-editing/","publishdate":"2021-06-21T10:35:00+08:00","relpermalink":"/project/gan-editing/","section":"project","summary":"GAN-based image attribute editing","tags":[],"title":"基于GAN的图片属性编辑","type":"project"},{"authors":["向孙程","hao-chen","mengyuan-guan"],"categories":[],"content":"背景介绍 行人重识别（Person re-identification, Re-ID)，也称为行人再识别或跨镜追踪；主要实现从一个摄像头捕获的目标行人，到其他不同摄像头检索是否存在相同行人，即进行跨摄像头检索。\n应用场景 行人重识别技术可以弥补目前固定摄像头的视觉局限, 并可与行人检测、行人跟踪技术相结合, 应用于视频监控、智能安防等领域。\n已有成果 行人重识别领域由于安全隐私、数据可获取性、标注难度等因素的限制，高质量大规模的真实标注数据依然非常稀缺。针对这类问题，我们针对具体任务场景，利用GTA-5游戏引擎自动构建了多属性有标注数据集。 来自标注数据集的行人样本 已发表论文 [1] Xiang S, Fu Y, You G, et al. Unsupervised domain adaptation through synthesis for person re-identification[C]//2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2020: 1-6. Link\n[2] Xiang S, Fu Y, You G, et al. Taking A Closer Look at Synthesis: Fine-Grained Attribute Analysis for Person Re-Identification[C]//ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 3765-3769. Link\n","date":1624242600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624242600,"objectID":"cbfa17ff77cfd74b2a3732572f8f4f37","permalink":"http://localhost:1313/project/reid/","publishdate":"2021-06-21T10:30:00+08:00","relpermalink":"/project/reid/","section":"project","summary":"Person re-identification","tags":[],"title":"行人重识别","type":"project"},{"authors":["于泽芳","yangcheng-li"],"categories":[],"content":"背景介绍 近年来，基于骨架的动作识别由于其对动态环境和复杂背景的鲁棒性而备受关注，其主要任务为根据输入的一段骨架序列，输出当前对应的动作类别。主流方法通常将关节点数据构建为图（graph）结构，并将图卷积扩展为时空图卷积来提取运动信息。\n应用场景 基于骨架的动作识别可以服务于人机交互、机器人、VR游戏实时控制等领域。\n已有成果 构建动作识别数据集时往往需耗费大量的人力物力，缺乏多视角下的动作序列。为解决这一问题，我们基于GTA V游戏构建了大规模360度全视角的虚拟动作数据集GAR-60。 GAR-60数据集部分样本 ","date":1624240800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624240800,"objectID":"93d53f0507ba328d32a843139c42df93","permalink":"http://localhost:1313/project/skeleton/","publishdate":"2021-06-21T10:00:00+08:00","relpermalink":"/project/skeleton/","section":"project","summary":"Skeleton-based human action recognition","tags":[],"title":"基于骨架的人类动作识别","type":"project"},{"authors":["Feng Wang","向孙程","刘婷","付宇卓"],"categories":[],"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624256645,"objectID":"c6fd4297aa55ba5e4109ac5f5a9e7966","permalink":"http://localhost:1313/publication/wang2021attention/","publishdate":"2021-06-21T06:24:05.111767Z","relpermalink":"/publication/wang2021attention/","section":"publication","summary":"Facial expression manipulation has two objectives: 1) generating an image with target expression; 2) preserving the identity information of the original image as much as possible. Recently, Generative Adversarial Networks (GANs) have shown the abilities for fine-grained facial expression manipulation. However, current methods are still prone to generate images with poor quality. In this work, we propose a U-Net based generator with multi-attention gate for facial expression manipulation. The multi-level attention mechanism is helpful to manipulate relevant regions and preserve identity features, thus improving the editing ability. Furthermore, we adopt self-attention block to replace direct skip-connection to get long-range dependency in images. To suppress artifacts in generated images, we add a discriminator based loss function in the training process. Extensive experiments on both quantitative and qualitative evaluation show that our proposed method achieves better performance for facial expression manipulation.","tags":[],"title":"Attention Based Facial Expression Manipulation","type":"publication"},{"authors":[],"categories":null,"content":"上海交通大学微纳电子学系2022级 直硕/直博 研究生优才夏令营通知\n报名时间：即日起至2021年6月28日\n欢迎广大学子报考iCAT实验室！\n","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624233600,"objectID":"95d119f9fb8fd8878dd070f83cac9048","permalink":"http://localhost:1313/post/21-06-21-recruit/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/21-06-21-recruit/","section":"post","summary":"欢迎加入iCAT实验室！","tags":[],"title":"招收2022级硕士研究生","type":"post"},{"authors":[],"categories":null,"content":"庆祝iCAT主页重新启动！ 请实验室各位成员及时补充个人相关信息！\n","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616025600,"objectID":"1c229b80af11f553450fc9b355e0fa6a","permalink":"http://localhost:1313/post/21-03-18-homepage-start/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/post/21-03-18-homepage-start/","section":"post","summary":"http://sjtu-icat.github.io","tags":[],"title":"iCAT主页启动","type":"post"},{"authors":[],"categories":null,"content":"生成器：利用随机采样的隐空间编码生成真实度极高的合成图片，例如StyleGAN等\n编码器：将真实图片转换为隐空间编码，以便于后续的编辑和生成\n预测器：判断生成图片的语义信息，例如人物的性别、是否微笑等\n属性编辑：对隐空间编码进行特定向量方向的操控，以达到对图片属性编辑的目的\n","date":1615294800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1615294800,"objectID":"bd6b0e8ac5bcab56230631e93202b16a","permalink":"http://localhost:1313/event/21-03-09-phd-pre/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/21-03-09-phd-pre/","section":"event","summary":"StyleGAN及其变体介绍","tags":[],"title":"博士组会汇报","type":"event"},{"authors":[],"categories":null,"content":" International Conference on Multimodla Interaction (ICMI)\nMay 2021 CCF-C International Conference on Neural Information Processing (ICONIP)\nJune 2021 CCF-C International Conference on Tools with Artifical Intelligence (ICTAI)\nJuly 2021 CCF-C ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1614556800,"objectID":"64db1730dc1ad59c6df7d6912fb79440","permalink":"http://localhost:1313/post/21-03-01-confence-list/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/21-03-01-confence-list/","section":"post","summary":"请大家努力！","tags":[],"title":"2021年上半年论文列表","type":"post"},{"authors":[],"categories":null,"content":"祝贺向孙程同学的论文\u0026quot;Taking a Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification\u0026quot;被ICASSP'21所录用！\n该会议属于CCF B类会议。\n","date":1611792000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1611792000,"objectID":"7723946348269c992c6e67b992052eaa","permalink":"http://localhost:1313/post/20-12-02-paper-accepted/","publishdate":"2021-01-28T00:00:00Z","relpermalink":"/post/20-12-02-paper-accepted/","section":"post","summary":"CCF B类会议","tags":[],"title":"祝贺向孙程同学的论文被ICASSP'21录用","type":"post"},{"authors":["向孙程","付宇卓","尤冠杰","刘婷"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624256645,"objectID":"12ad3aa9f306b0cdffc7005003a7171a","permalink":"http://localhost:1313/publication/xiang-2021-taking/","publishdate":"2021-06-21T06:24:05.111767Z","relpermalink":"/publication/xiang-2021-taking/","section":"publication","summary":"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has achieved remarkable performance. However, in pursuit of high accuracy, researchers in the academic always focus on training with large-scale datasets at a high cost of time and label expenses, while neglect to explore the potential of performing efficient training from millions of synthetic data. To facilitate development in this field, we reviewed the previously developed synthetic dataset GPR and built an improved one (GPR+) with larger number of identities and distinguished attributes. Based on it, we quantitatively analyze the influence of dataset attribute on re-ID system. To our best knowledge, we are among the first attempts to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. This research helps us have a deeper understanding of the fundamental problems in person re-ID, which also provides useful insights for dataset building and future practical usage.","tags":[],"title":"Taking A Closer Look at Synthesis: Fine-Grained Attribute Analysis for Person Re-Identification","type":"publication"},{"authors":[],"categories":null,"content":"使用服务器时请自觉遵守使用说明！\n服务器种类 .158：Nvidia Tesla P100 SXM2 （四核，16GB） .249：Nvidia RTX 3090*4 （单核，24GB） 本地：Nvidia RTX 1080 Ti （单核，11GB） 载入服务器模块 每次登入服务器时均需要加载\n显示当前可用的软件清单 module av\n加载常用模块 module load anaconda3/2019.10 #亦可在自己路径下独立安装anaconda module load cuda/11.1 module load cudnn/8.0.5 module load gcc/7.3.0 conda简要使用说明 初始化conda shell conda init bash\n创建虚拟环境（以testenv举例） conda create -n testenv\n激活虚拟环境 conda activate testenv\n安装软件包 conda install numpy\nconda install pytorch=0.4.0 # 指定具体版本\n退出当前的虚拟环境 conda deactivate\n查看所有的虚拟环境 conda env list\n查看当前虚拟环境安装包 conda list\n删除虚拟环境及其下面的所有包 conda remove -n testenv —-all\n其他说明 软件最低版本需求 在3090上低于该版本将无法正常运行需要GPU的程序\nPyTorch\u0026gt;=1.7.0 Tensorflow\u0026gt;=2.4.0 服务器资源的分配 使用nvidia-smi或者gpustat(需使用pip或conda安装)查看当前服务器GPU使用状态 多数程序运行时会默认占用所有可用的GPU，需要长时间使用GPU时，在运行指令前添加CUDA_VISIBLE_DEVICES指定具体运行的GPU，例如CUDA_VISIBLE_DEVICES=0 python main.py即指定在0号GPU上运行程序 在服务器资源紧张时，将会根据任务优先级对资源进行协调（例如近期需要投会议或期刊的同学可以优先使用） 请注意自己home路径下的硬盘占用情况，定期清理临时文件 ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1609459200,"objectID":"b673e3c677f0f016bab2009c0b65feae","permalink":"http://localhost:1313/post/21-01-01-server-usage/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/post/21-01-01-server-usage/","section":"post","summary":"使用服务器时请自觉遵守使用说明！\n","tags":[],"title":"实验室服务器使用说明","type":"post"},{"authors":["向孙程","付宇卓","尤冠杰","刘婷"],"categories":[],"content":"","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616139643,"objectID":"6d3c514d0925c5b021173daec2154446","permalink":"http://localhost:1313/publication/xiang-2020-unsupervised/","publishdate":"2020-07-06T07:40:42.885369Z","relpermalink":"/publication/xiang-2020-unsupervised/","section":"publication","summary":"Person re-identification is a hot topic because of its widespread applications in video surveillance and public security. However, it remains a challenging task because of drastic variations in illumination or background across surveillance cameras, which causes the current methods can not work well in real-world scenarios. In addition, due to the scarce dataset, many methods suffer from over-fitting to a different extent. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic random scenes and simultaneously annotate them without any manpower. Based on it, we build a large-scale, diverse synthetic dataset. Secondly, we propose a novel unsupervised Re-ID method via domain adaptation, which can exploit the synthetic data to boost the performance of re-identification in a completely unsupervised way, and free humans from heavy data annotations. Extensive experiments show that our proposed method achieves the state-of-the-art performance on two benchmark datasets, and is very competitive with current cross-domain Re-ID method.","tags":[],"title":"Unsupervised domain adaptation through synthesis for person re-identification","type":"publication"},{"authors":["向孙程","付宇卓","尤冠杰","刘婷"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616155521,"objectID":"75b5e41b3a8b2101fd8ed0fb990742b6","permalink":"http://localhost:1313/publication/xiang-2020-attribute/","publishdate":"2021-03-19T12:05:20.585767Z","relpermalink":"/publication/xiang-2020-attribute/","section":"publication","summary":"","tags":[],"title":"Attribute analysis with synthetic dataset for person re-identification","type":"publication"},{"authors":["向孙程","付宇卓","刘婷"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1616140988,"objectID":"9177e08ee4cd4a71d1d93c4a286ce6ac","permalink":"http://localhost:1313/publication/xiang-2019-deep/","publishdate":"2019-11-04T08:03:08.384351Z","relpermalink":"/publication/xiang-2019-deep/","section":"publication","summary":"The superiority of deeply learned representation has been reported in very recent literature of re-identification (Re-ID) task. In this paper, we study a novel transfer learning problem termed Distant Domain Transfer Learning (DDTL) for Re-ID task. Different from existing transfer learning problems which assume that there is a close relation between source domain and target domain, in the DDTL problem, target domain can be totally different from source domain. For example, the source domain classifies pedestrian images but the target domain distinguishes vehicle images. In this work, our goal is to execute an unseen and unrelated task based on a labeled dataset training previously without any samples from intermediate domains. Particularly, we consider the more pragmatic issue of learning a deep feature with no labels, and propose a Deep Unsupervised Progressive Learning (DUPL) method to transfer pretrained deep representations to unseen domains. Specifically, our work performs clustering and fine-tuning of the CNN to improve the performance of original model trained on the irrelevant labeled dataset. Empirical studies on distant domain adaptation task (pedestrian -\u003e vehicle) demonstrate the effectiveness of the proposed method, and the improvement in terms of the mAP accuracy is up to 15% over \"non-transfer\" methods.","tags":[],"title":"Deep unsupervised progressive learning for distant domain adaptation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://localhost:1313/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"http://localhost:1313/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"关于我们","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"http://localhost:1313/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"实验室项目","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"http://localhost:1313/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"团队成员","type":"widget_page"}]